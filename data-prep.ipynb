{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-20T15:40:37.085857Z","iopub.status.busy":"2023-07-20T15:40:37.085515Z","iopub.status.idle":"2023-07-20T15:40:37.092200Z","shell.execute_reply":"2023-07-20T15:40:37.091079Z","shell.execute_reply.started":"2023-07-20T15:40:37.085830Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","\n","from datasets import Dataset, DatasetDict, Audio, load_from_disk, concatenate_datasets\n","from transformers import WhisperFeatureExtractor\n","from transformers import WhisperTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:40:37.094220Z","iopub.status.busy":"2023-07-20T15:40:37.093871Z","iopub.status.idle":"2023-07-20T15:40:37.104164Z","shell.execute_reply":"2023-07-20T15:40:37.103347Z","shell.execute_reply.started":"2023-07-20T15:40:37.094195Z"},"trusted":true},"outputs":[],"source":["TRAINING_CSV_PATH=\"./bengaliai-speech/train.csv\"\n","TRAINING_AUDIO_LOCATION=\"./bengaliai-speech/train_mp3s/\"\n","BASE_MODEL=\"openai/whisper-tiny\"\n","LANGUAGE=\"bengali\"\n"]},{"cell_type":"markdown","metadata":{},"source":["# Import and pre-process the data for training and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:40:37.148276Z","iopub.status.busy":"2023-07-20T15:40:37.147235Z","iopub.status.idle":"2023-07-20T15:40:39.888092Z","shell.execute_reply":"2023-07-20T15:40:39.887156Z","shell.execute_reply.started":"2023-07-20T15:40:37.148240Z"},"trusted":true},"outputs":[],"source":["raw_data = pd.read_csv(f\"{TRAINING_CSV_PATH}\")\n","print(f\"Number of Samples in Set: {len(raw_data)}\")\n","raw_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["**split the training and validation data into two sets**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:40:39.890620Z","iopub.status.busy":"2023-07-20T15:40:39.889733Z","iopub.status.idle":"2023-07-20T15:40:40.376371Z","shell.execute_reply":"2023-07-20T15:40:40.375450Z","shell.execute_reply.started":"2023-07-20T15:40:39.890549Z"},"trusted":true},"outputs":[],"source":["def create_split(df, col, value):\n","    split = df[df[col] == value]\n","    split = split[['id', 'sentence']]\n","    split['audio'] = split['id'].apply(lambda x: f\"{TRAINING_AUDIO_LOCATION}{x}.mp3\")\n","    return split\n","\n","train_df = create_split(raw_data, 'split', 'train')\n","validation_df = create_split(raw_data, 'split', 'valid')\n","\n","print(f\"{len(train_df)} entries in the training set\")\n","print(f\"{len(validation_df)} entries in the validation set\")\n","train_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Create a hugging face Dataset and DatasetDict**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:40:40.378049Z","iopub.status.busy":"2023-07-20T15:40:40.377790Z","iopub.status.idle":"2023-07-20T15:40:41.247117Z","shell.execute_reply":"2023-07-20T15:40:41.245552Z","shell.execute_reply.started":"2023-07-20T15:40:40.378028Z"},"trusted":true},"outputs":[],"source":["train_df = train_df.head(10000)\n","validation_df = validation_df.head(1000)\n","\n","train_ds = Dataset.from_dict({\"audio\": train_df['audio'], \"sentence\": train_df['sentence']}).cast_column(\"audio\", Audio(sampling_rate=16000))\n","validation_ds = Dataset.from_dict({\"audio\": validation_df['audio'], \"sentence\": validation_df['sentence']}).cast_column(\"audio\", Audio(sampling_rate=16000))\n","\n","# datasets = DatasetDict({'train': train_ds, 'validation': validation_ds})"]},{"cell_type":"markdown","metadata":{},"source":["**Create the whisper tokenizer and extractor**\n","*check to validate that tokenizer is working*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:40:41.250161Z","iopub.status.busy":"2023-07-20T15:40:41.249814Z","iopub.status.idle":"2023-07-20T15:40:41.295689Z","shell.execute_reply":"2023-07-20T15:40:41.294461Z","shell.execute_reply.started":"2023-07-20T15:40:41.250131Z"},"trusted":true},"outputs":[],"source":["\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(BASE_MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:40:41.297067Z","iopub.status.busy":"2023-07-20T15:40:41.296803Z","iopub.status.idle":"2023-07-20T15:40:41.407105Z","shell.execute_reply":"2023-07-20T15:40:41.406125Z","shell.execute_reply.started":"2023-07-20T15:40:41.297045Z"},"trusted":true},"outputs":[],"source":["\n","tokenizer = WhisperTokenizer.from_pretrained(BASE_MODEL, language=LANGUAGE, task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:40:41.408378Z","iopub.status.busy":"2023-07-20T15:40:41.408146Z","iopub.status.idle":"2023-07-20T15:40:41.423390Z","shell.execute_reply":"2023-07-20T15:40:41.421995Z","shell.execute_reply.started":"2023-07-20T15:40:41.408356Z"},"trusted":true},"outputs":[],"source":["input_str = raw_data.iloc[0]['sentence']\n","labels = tokenizer(input_str).input_ids\n","decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n","decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n","\n","print(f\"Input String: {input_str}\")\n","print(f\"Decoded w/ Special: {decoded_with_special}\")\n","print(f\"Decoded w/o Special: {decoded_str}\")\n","print(f\"Are Equal: {input_str == decoded_str}\")"]},{"cell_type":"markdown","metadata":{},"source":["**Prepare the dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:40:41.424755Z","iopub.status.busy":"2023-07-20T15:40:41.424533Z"},"trusted":true},"outputs":[],"source":["def prepare_dataset(batch):\n","    # load and resample audio data from 48 to 16kHz\n","    audio = batch[\"audio\"]\n","\n","    # compute log-Mel input features from input audio array \n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # encode target text to label ids \n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch\n","\n","# shard the training datasets into chunks for preperation\n","num_shards = 10\n","for shard_idx in range(num_shards):\n","    shard = train_ds.shard(num_shards=num_shards, index=shard_idx, contiguous=True)\n","    shard.save_to_disk(f\"./training-shards/bengali-ai-train-set-{shard_idx}\")\n","\n","# load the shards and prepare them\n","prepared_training_datasets = []\n","for shard_idx in range(num_shards):\n","    shard = load_from_disk(f\"./training-shards/bengali-ai-train-set-{shard_idx}\")\n","    shard = shard.map(prepare_dataset, remove_columns=shard.column_names, writer_batch_size=1000, keep_in_memory=False)\n","    prepared_training_datasets.append(shard)\n","    \n","for shard_idx in range(num_shards):\n","    shard = validation_ds.shard(num_shards=num_shards, index=shard_idx, contiguous=True)\n","    shard.save_to_disk(f\"./valid-shards/bengali-ai-valid-set-{shard_idx}\")\n","\n","# load the shards and prepare them\n","prepared_validation_datasets = []\n","for shard_idx in range(num_shards):\n","    shard = load_from_disk(f\"./valid-shards/bengali-ai-valid-set-{shard_idx}\")\n","    shard = shard.map(prepare_dataset, remove_columns=shard.column_names, writer_batch_size=1000, keep_in_memory=False)\n","    prepared_validation_datasets.append(shard)\n","\n","# concatenate the prepared shards into a single dataset\n","datasets = DatasetDict({\"valid\": concatenate_datasets(prepared_validation_datasets), \"train\": concatenate_datasets(prepared_training_datasets)})\n","\n","print(datasets)\n","\n","# print(\"Preparing dataset\")\n","# datasets = datasets.map(prepare_dataset, remove_columns=datasets.column_names[\"train\"], writer_batch_size=1000, keep_in_memory=False)\n","datasets.push_to_hub(\"bengali-ai-train-set-tiny\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
